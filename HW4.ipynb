{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of HW4new.ipynb","provenance":[{"file_id":"11CmjcOpjWeZSOOr7TEmV5HbFIYx2EZe3","timestamp":1588863979233},{"file_id":"1S2IocRQdDE09i5KR6ZCchdji6SEbVUTc","timestamp":1588836539424},{"file_id":"1gYhf63p6lyrvoXLQ6NipjpfNADWVKRvk","timestamp":1588636925965}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S4Iui5UetZvC","colab_type":"text"},"source":["#**HW4 : Recurrent Neural Networks**\n","In this homework, we will explore how to develop a simple Recurrent Neural Network (RNN) for sentiment analysis. As a dataset, we will use the IMDB dataset. It contains the text of some reviews and the sentiment given by the authors of the reviews (positive review or negative review). The input to the RNN is the sequence of words that compose a review. The learning task consists in predicting the sentiment of the review.\n","In the first part, we will learn how to develop a simple RNN, then we will explore the differences in terms of computational load, number of parameters, and performances with respect to more advanced recurrent models, like LSTM and GRU. Subsequently, we experiment with the bi-directional model to unveil the strengths and the weaknesses of this technique. Finally, we will explore how to face overfitting by Dropout. "]},{"cell_type":"markdown","metadata":{"id":"WTXPA4gUKkpp","colab_type":"text"},"source":["##Simple RNN\n","\n","Let's start by importing Tensorflow, Keras and Numpy"]},{"cell_type":"code","metadata":{"id":"G-hX557NDQ9s","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","\n","np.random.seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZxFjnG7Kuwi","colab_type":"text"},"source":["###Load dataset:\n","In this HW, we use the IMD dataset that can be easily downloaded using Keras. The dataset contains 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes. For convenience, the words are indexed by the overall frequency in the dataset, so that for example the integer \"3\" encodes the 3rd most frequent word in the data. For testing purposes, we will only consider the first 10,000  most common words.\n","By default, the load_data method returns a breakdown of the dataset into training and test sets. Both these sets contain 25,000 samples. To also have a validation set, we split the test set in half."]},{"cell_type":"code","metadata":{"id":"NpaX6AGOD77D","colab_type":"code","colab":{}},"source":["\n","num_words = 10000\n","(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)\n","(X_valid, X_test) = X_test[:12500], X_test[12500:]\n","(y_valid, y_test) = y_test[:12500], y_test[12500:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZHxa95qQ0jb","colab_type":"text"},"source":["Let's also get the word indexs (word to word-id)\n","\n","*   List item\n","*   List item\n","\n"]},{"cell_type":"code","metadata":{"id":"LszZMh45X8Wz","colab_type":"code","colab":{}},"source":["word_index = keras.datasets.imdb.get_word_index()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxLLVb5mRc0v","colab_type":"text"},"source":["Now we create a reverse index (word-id to word) method. Moreover, we add three special word-ids to encode:\n","- the padding;\n","- the start of a sequence;\n","- a word that is not in the vocabulary of the first 10,000 most common words.\n","\n","Moreover, we also add an \"unknown\" placeholder for all the other symbols (not words) that may occur.\n","*Please, notice that Keras does not use index 0, so we can shift the indices only by 3 positions.*"]},{"cell_type":"code","metadata":{"id":"NeZwDtDjYFt3","colab_type":"code","colab":{}},"source":["reverse_index = {word_id + 3: word for word, word_id in word_index.items()}\n","reverse_index[0] = \"<pad>\" # padding\n","reverse_index[1] = \"<sos>\" # start of sequence\n","reverse_index[2] = \"<oov>\" # out-of-vocabulary\n","reverse_index[3] = \"<unk>\" # unknown\n","\n","def decode_review(word_ids):\n","    return \" \".join([reverse_index.get(word_id, \"<err>\") for word_id in word_ids])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIMY115nRzXQ","colab_type":"text"},"source":["Let's print a training sample and its target value"]},{"cell_type":"code","metadata":{"id":"jnBRMiZJYQpm","colab_type":"code","outputId":"28ed4005-1b71-46b9-9bd2-c7f6f4c63f6f","executionInfo":{"status":"ok","timestamp":1588632280082,"user_tz":-120,"elapsed":623,"user":{"displayName":"Luca Pasa","photoUrl":"","userId":"03099207101535299227"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["decode_review(X_train[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"<sos> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <oov> is an amazing actor and now the same being director <oov> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <oov> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <oov> to the two little boy's that played the <oov> of norman and paul they were just brilliant children are often left out of the <oov> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"J1tzUWldYT8X","colab_type":"code","outputId":"6ae8bcb3-de90-48c9-efeb-2cb019b8d071","executionInfo":{"status":"ok","timestamp":1588632281622,"user_tz":-120,"elapsed":627,"user":{"displayName":"Luca Pasa","photoUrl":"","userId":"03099207101535299227"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_train[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"0O7x_P0IqihK","colab_type":"text"},"source":["Because of a limit of Keras, to feed the input data into an RNN model we have to create sequences that have the same length. We use keras.preprocessing.sequence.pad_sequences() to preprocess X_train: this will create a 2D array of 25,000 rows (one per review) and maxlen=500 columns. Because of that, reviews longer than 500 words will be cut, while reviews shorter than 500 words will be padded with zeros."]},{"cell_type":"code","metadata":{"id":"wtMUMGMUYdAA","colab_type":"code","colab":{}},"source":["maxlen = 500\n","X_train_trim = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n","X_test_trim = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n","X_valid_trim = keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jLxP0KW1104","colab_type":"text"},"source":["###Define the model:\n","Let's define the model: \n","- The first layer is an Embedding layer, with input_dim=num_words and output_dim=10. The model will gradually learn to represent each of the 10,000 words as a 10-dimensional vector. So the next layer will receive 3D batches of shape (batch size, 500, 10)\n","- The second layer is the recurrent one. In particular, in this case, we use a [SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) \n","- The output layer is a Dense layer with a sigmoid activation function since this is a binary classification problem"]},{"cell_type":"code","metadata":{"id":"xPnihHKiYf__","colab_type":"code","colab":{}},"source":["model = keras.models.Sequential()\n","model.add(keras.layers.Embedding(input_dim=num_words, output_dim=10))\n","model.add(keras.layers.SimpleRNN(32))\n","model.add(keras.layers.Dense(1, activation=\"sigmoid\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZXBW-1w2nSG","colab_type":"text"},"source":["Since the model performs a binary classification task, we use cross-entropy as loss."]},{"cell_type":"code","metadata":{"id":"AcoAaVy_YiM4","colab_type":"code","colab":{}},"source":["model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IBNh_o2I2SN","colab_type":"text"},"source":["Let's print a summary of the model. Specifically, note the number of parameters of the RNN layer."]},{"cell_type":"code","metadata":{"id":"6AKJ1qsgYkH7","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTrvel3KRkYy","colab_type":"text"},"source":["###Train the model:\n","Now we have to train the model"]},{"cell_type":"code","metadata":{"id":"zCqSgX9WYmaR","colab_type":"code","colab":{}},"source":["history = model.fit(X_train_trim, y_train,\n","                    epochs=10, batch_size=128, validation_data=(X_valid_trim, y_valid))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Fvscg67R4H1","colab_type":"text"},"source":["Print the values of accuracy and the loss , and evaluate the model on the test set"]},{"cell_type":"code","metadata":{"id":"GKXRHieZbu-2","colab_type":"code","colab":{}},"source":["def plot_loss(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch,history.history['loss'])\n","  plt.plot(history.epoch,history.history['val_loss'])\n","  plt.title('loss')\n","\n","def plot_accuracy(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch,history.history['accuracy'])\n","  plt.plot(history.epoch,history.history['val_accuracy'])\n","  plt.title('accuracy')\n","\n","plot_loss(history)\n","\n","plot_accuracy(history)\n","\n","scores = model.evaluate(X_test_trim, y_test, verbose=2)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHc_4yOASBXu","colab_type":"text"},"source":["##Exercise 4.1: LSTM and GRU\n","**To complete**: In this Exercise, you have to implement 2 models, similar to the previous one that, instead of exploiting the RNN layer, use an LSTM and a GRU Layer, respectively. For each model print the summary. Then, train it and plot the values of accuracy and loss. Finally, discuss the differences in terms of performance, the number of parameters, and training time. Note that you can use a different number of units than the one used in the RNN example.\n","\n","**To complete**: In order to perform a fair comparison (definition of fair: models have to use more or less the same number of parameters) between the given RNN model and the other 2 models (LSTM and GRU), how many units do they have to use, respectively?\n","\n","*insert cells (code and text) with results and discussion below*"]},{"cell_type":"markdown","metadata":{"id":"g2JZMtVATw5T","colab_type":"text"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","##Bidirectional LSTM\n","Let's modify the previous code by using a bidirectional LSTM instead of a simple LSTM. In Keras, it is possible to define a bidirectional layer by using [tf.keras.layers.Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional). Note that this wrapper requires as argument a layer, in our case we use [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n"]},{"cell_type":"code","metadata":{"id":"svybeLNAedBf","colab_type":"code","colab":{}},"source":["model_bidirectional = keras.models.Sequential()\n","model_bidirectional.add(keras.layers.Embedding(input_dim=num_words, output_dim=10))\n","model_bidirectional.add(keras.layers.Bidirectional(keras.layers.LSTM(32)))\n","model_bidirectional.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n","\n","model_bidirectional.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model_bidirectional.summary()\n","\n","history = model_bidirectional.fit(X_train_trim, y_train,\n","                    epochs=10, batch_size=128, validation_split=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qICZr3WpfNI8","colab_type":"code","colab":{}},"source":["plot_loss(history)\n","\n","plot_accuracy(history)\n","\n","scores = model_bidirectional.evaluate(X_test_trim, y_test, verbose=2)\n","print(\"%s: %.2f%%\" % (model_bidirectional.metrics_names[1], scores[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8hmkhTciu89","colab_type":"text"},"source":["##Exercise 4.2: Dropout\n","In the bi-directional model is it possible to notice that the model overfits the training data.\n","A possible solution to this problem could be applying the dropout technique to input or output layers.\n","\n","In Keras, to apply dropout you have to add a [keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layer.\n","\n","**To complete**: modify the code of the previous exercise adding the Dropout to the input and/or output layers. Find the Dropout probability that reduces the overfitting issue. Is it possible to modify the values of some other hyper-parameters to mitigate overfitting? And if this is the case, explain why and perform an experimental study demonstrating your point.\n","\n","*insert cells (code and text) with results and discussion below*"]}]}