{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUc9frYnVBI_",
        "colab_type": "text"
      },
      "source": [
        "#**HW 2: Optimize and train Deep Models**\n",
        "\n",
        "In this homework, we will explore how to develop a simple Deep Neural Network for image classification. We will explore two common libraries: TensorFlow and Keras.\n",
        "Then we will explore how to face a well know problem that is common to encounter during the training phase: the Overfitting.\n",
        "Finally, we will study how to perform a fair model selection.\n",
        "Hint: Before starting the exercise take a look at how Tensorflow and Keras are designed.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Nc7mFSVEiU",
        "colab_type": "text"
      },
      "source": [
        "##Exercise 2.1: Image Classficiation with Tensorflow and Keras\n",
        "\n",
        "In this first exercise we will develop a 3 layer Neural Network to perfrom image classification.\n",
        "\n",
        "Let's start importing the libraries we will need and setting a couple of environmental variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWn9FzneVqJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras  # tf.keras\n",
        "import time\n",
        "\n",
        "#Set the log level in order to hide the unuseful warnings\n",
        "import logging\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
        "\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT2HMYDEXcx0",
        "colab_type": "text"
      },
      "source": [
        "### Load Data: Fashion MNIST dataset\n",
        "\n",
        "We will use the Fashion MNIST dataset, a dataset of Zalando's articles. Each sample is a 28x28 pixels grayscale image, associated with a label from 10 classes:\n",
        "\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "Each pixel intensity is represented by a uint8 (byte) from 0 to 255.\n",
        "We will divide the dataset in training, testing and validation set. As you already know, the training set will be used to train the model, the validation set will be used to perform model selection and finally, the test set will be used to asses the performance of deep network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZv86gBLXj-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = (\n",
        "    fashion_mnist.load_data()) #The dataset is already divede in test and training...\n",
        "\n",
        "#We extract the first 5000 samples of the training set, to use them as the validation set\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:] \n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t71N3VCLYzpK",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at a sample of the images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw_1HUI6YyRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_rows = 5\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols*1.4, n_rows * 1.6))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGUg8TexZMNf",
        "colab_type": "text"
      },
      "source": [
        "### Define the model:\n",
        "\n",
        "Let's build a Sequential model (keras.models.Sequential) and add four layers to it by calling its add() method:\n",
        "\n",
        "\n",
        "*  we start with a Flatten layer (keras.layers.Flatten) to convert each 28x28 image to a single row of 784 pixel values. Since it is the first layer in your model, you should specify the input_shape argument, leaving out the batch size: [28, 28];\n",
        "*  a Dense layer (keras.layers.Dense) with 300 neurons, and the \"relu\" activation function;\n",
        "* another Dense layer with 100 neurons, also with the \"relu\" activation function;\n",
        "* a final Dense layer with 10 output neurons (one per class), and with the \"softmax\" activation function to ensure that the sum of all the estimated class probabilities for each image is equal to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaoEISyaZL56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYfxUOQVZljh",
        "colab_type": "text"
      },
      "source": [
        "Call the model's summary() method to check if the model has been built correctly. Also, try using keras.utils.plot_model() to save an image of your model's architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpGkQl5dZoWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1NbkrXRZwOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, \"my_mnist_model.png\", show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc3xzuA-33WH",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the first dimension of each layer is not defined (?), indeed this dimension in each layer is variable and depends on the batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ySdJmzxaSuz",
        "colab_type": "text"
      },
      "source": [
        "### Optimize the model:\n",
        "\n",
        "After the model has been created, you must call its compile() method to specify the loss function and the optimizer to use.\n",
        "\n",
        "The conventional way is to have the target outputs to match the output shape. In this case that would mean to convert the targets to the one-hot encoded array to match with the output shape.\n",
        "However, with the help of the sparse_categorical_crossentropy loss function, we can skip that step and keep the integers as targets.\n",
        "\n",
        " In this case, we want to use the \"sparse_categorical_crossentropy\" loss, and the SGD optimizer (stochastic gradient descent). \n",
        "\n",
        "\n",
        "\n",
        "Moreover, you can optionally specify a list of additional metrics that should be measured during training. In this case we specify metrics=[\"accuracy\"]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3bbsY3uaeh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB9SV0MpaeTg",
        "colab_type": "text"
      },
      "source": [
        "Now our model is ready to be trained. Call its fit() method, passing to it the input features (X_train) and the target classes (y_train). Set the number of epoch to 20. \n",
        "In order to validate our model we will also pass the validation data by setting validation_data=(X_valid, y_valid). Keras will compute the loss and the additional metrics (the accuracy in this case) on the validation set at the end of each epoch. If the loss on the training set is much lower than the one on the validation set, your model is probably overfitting the training set. Note: the fit() method will return a History object containing training stats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5whx-LU3aoKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPxiw8iCADr-",
        "colab_type": "text"
      },
      "source": [
        "Let's plot the loss and the accuracy trends on both training and validation sets. We define the \"plot_learning_curves\" because we will reuse it in the next steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJg-F7mUa7st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_learning_curves(history):\n",
        "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(0, 1)\n",
        "    plt.show()\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O3-sLmxansJ",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model:\n",
        "\n",
        "Now, our model has been optimized on the training set, and as you can see the performance on the validation set in quite similar (so it does not overfit the training data). Let's now evaluate the performance of our model using the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-ZvJEcObdyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Xbey3u6yss",
        "colab_type": "text"
      },
      "source": [
        "### Input normalization:\n",
        "\n",
        "When using Gradient Descent, it is usually a good idea to ensure that all the features have a similar scale. It's a common practice to scale your data inputs to have zero mean and unit variance. This is known as the standard scaler approach.\n",
        "Try to standardize the pixel values and see if this improves the performance of your neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNFVWb8iTigz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To complete: define X_train_scaled, X_valid_scaled and X_test_scaled, the sets \n",
        "#that contain the normalized inputs.\n",
        "#Hint: For each feature (pixel intensity), you must subtract the mean() of that \n",
        "#feature (across all instances, so use axis=0) and divide by its standard \n",
        "#deviation (std(), again axis=0)\n",
        "\n",
        "X_train_scaled = #To complete \n",
        "X_valid_scaled = #To complete\n",
        "X_test_scaled = #To complete\n",
        "\n",
        "#Make sure you compute the means and standard deviations on the training set,\n",
        "#and use these statistics to scale the training set, the validation set and the\n",
        "# test set\n",
        "\n",
        "#Alternatively, you can use Scikit-Learn's StandardScaler..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqd4Xw9UTwy2",
        "colab_type": "text"
      },
      "source": [
        "Let's define the model in order to re-initialize the weights and then train it by using the normalize data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmhZXTqHTy-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, epochs=20,\n",
        "                    validation_data=(X_valid_scaled, y_valid))\n",
        "print(\"Model Evaluation\")\n",
        "model.evaluate(X_test_scaled, y_test)\n",
        "plot_learning_curves(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l8V4fSRVc6D",
        "colab_type": "text"
      },
      "source": [
        "Note that the validation curve is smoother than in the previous test and it is also more simlar to the training curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeGlzKe864Gq",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2.2: Overfiting\n",
        "\n",
        "A common problem that occurs when you train a deep neural network is overfittig. Overfitting occurs when you achieve a good fit of your model on the training data, while it does not generalize well on new, unseen data. In other words, the model learned patterns specific to the training data, which are irrelevant in other data.\n",
        "As we have seen in the previous exercise, our model does not overfit the training data. In this exercise, we try to modify the training parameters in order to have a model that overfits.\n",
        "Overfitting can have many causes and usually is a combination of some of them, for instance: too many parameters/ layers, too few training samples, wrong learning rate (usualy too high), etc.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2B2DFEC8f88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To complete: modify the previuos model in order to obtain  a new mdel that overfit the training data\n",
        "#HINT: A model tends to overfit when too few training examples are available\n",
        "X_train_small = #To complete\n",
        "y_train_small = #To complete\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "\n",
        "#To complete: Add layers\n",
        "#HINT: A model tends to overfit  when the number of learnable parameters is very high compared to the number of training samples\n",
        "#HINT: Is it possible to use different activation functions than ReLu\n",
        "\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=To_complete)) #To complete:learning rate\n",
        "\n",
        "#HINT: A model tends to overfit when the number of training epochs is high\n",
        "history = model.fit(X_train_small, y_train_small, epochs=To_complete, #To complete: number of epochs\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "print(\"Model Evaluation\")\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS0fujEqh_m3",
        "colab_type": "text"
      },
      "source": [
        "Define the plot_learning_loss function and plot the loss and val_loss trends"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5EsklC0DZYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_learning_loss(history):\n",
        "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "plot_learning_loss(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW4Kf4fgDfOs",
        "colab_type": "text"
      },
      "source": [
        "### L1-norm\n",
        "\n",
        "One possible way to solve the overitting issue is by using regularization methods. The two most common regularization methods in Deep Learning are the L1-norm regularization and the L2-norm regularization. Both These techniques are based on limiting the capacity of models, by adding a parameter norm penalty to the objective function $\\mathcal{J}$:\n",
        "$$\n",
        "\\hat{\\mathcal{J}}(\\theta,\\mathbf{X},\\mathbf{y}) = \\mathcal{J}(\\theta,\\mathbf{X},\\mathbf{y}) + \\alpha \\Omega(\\theta)\n",
        "$$\n",
        "where $\\alpha$ is a hyperparameter that weighs the relative contribution of the norm penalty $\\Omega$.\n",
        "Lets start by considering the L1-norm regularization where the regularization term is defined as:\n",
        "$$\n",
        " \\Omega(\\theta)=||\\mathbf{W}||_1=\\sum_i |\\mathbf{w}|\n",
        "$$\n",
        "Let's find the values for the $\\alpha$ parameters that allow removing remove the overfitting effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzD8520dDgw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "\n",
        "#In Keras is neccesary to add the regularizer by using the attribute kernel_regularizer\n",
        "#to each layer whose weights will be considered in the Omega function.\n",
        "#is it also possibile to consider the bias by using the attribute bias_regularizer \n",
        "#tf.keras.regularizers.l1(alpha) perfrom the L1-norm regularization\n",
        "\n",
        "#To complete: insert the model that you previously define that overfit the \n",
        "#trainnig data, and add the L1-norm regularization. Use the proper value for the\n",
        "#alpha hyper-parameter, that prevents the model from overfitting. In general, a\n",
        "#layer that exploits L1-norm regularization is defined as follow:\n",
        "\n",
        "#model.add(keras.layers.Dense(units=To_complete, activation=To_complete,kernel_regularizer=tf.keras.regularizers.l1(To_complete))) \n",
        "\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(To_complete)) #To complete: learning rate\n",
        "\n",
        "history = model.fit(X_train_small, y_train_small, epochs=To_complete, #To complete: number of epochs\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "print(\"Model Evaluation\")\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_learning_loss(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgnCGYUvDqbC",
        "colab_type": "text"
      },
      "source": [
        "### L2-norm\n",
        "\n",
        "L2-norm regularization is also known as weight decay. This strategy drives the weights closer to the origin by adding the regularization term omega which is defined as:\n",
        "$$\n",
        " \\Omega(\\theta)=\\frac{1}{2}||\\mathbf{W}||_2^2\n",
        "$$\n",
        "\n",
        "Let's find the values for the $\\alpha$ parameters that allow removing remove the overfitting effect with L2-norm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqNhu350DtJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "\n",
        "#To complete: insert the model that you previously define that overfit the \n",
        "#trainnig data, and add the L2-norm regularization. Use the proper value for the\n",
        "#alpha hyper-parameter, that prevents the model from overfitting. In general, a\n",
        "#layer that exploits L2-norm regularization is defined as follow:\n",
        "\n",
        "#model.add(keras.layers.Dense(units=To_complete, activation=To_complete,kernel_regularizer=tf.keras.regularizers.l2(To_complete))) \n",
        "\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(To_complete)) #To complete: learning rate\n",
        "\n",
        "history = model.fit(X_train_small, y_train_small, epochs=To_complete, #To complete: that prevents the model from overfitting\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "print(\"Model Evaluation\")\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_learning_loss(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyKPb4DD25G",
        "colab_type": "text"
      },
      "source": [
        "### Early stopping\n",
        "\n",
        "Early Stopping is a form of regularization used to avoid overfitting. It is designed to monitor the generalization error of one model and stop training when generalization error begins to degrade. In order to evaluate the generalization error, early stopping requires that a validation dataset is evaluated during training. Then, when the validation error does not improve for a specific number of epochs (patience hyper-parameter), it stops the training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqNOOZ11D7cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "\n",
        "#To complete: insert the model that you previously define that overfit the \n",
        "#trainnig data\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=To_complete)) #To complete: learning rate\n",
        "\n",
        "#Let's define a log dir in order to save the checkpoint file\n",
        "logdir = os.path.join(os.curdir, \"my_logs\", \"run_{}\".format(time.time()))\n",
        "\n",
        "#In Keras the Eraly stopping is manage by using the callbacks argument.\n",
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(logdir),\n",
        "    keras.callbacks.EarlyStopping(patience=To_complete), #To complete: play with patience and find the value that prevents the model from overfitting\n",
        "    #Saving the checkpoints file allows to load the \"best\" model when the Early \n",
        "    #stopping detect that the generalization error degrade (after 'patience' epochs)\n",
        "    keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True),\n",
        "]\n",
        "\n",
        "history = model.fit(X_train_small, y_train_small, epochs=To_complete, #To complete: number of epochs\n",
        "                    validation_data=(X_valid_scaled, y_valid),\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "print(\"Model Evaluation\")\n",
        "#The early stopping stopped training after few epochs without progress, so your\n",
        "#model may already have started to overfit the training set. Since the \n",
        "#ModelCheckpoint callback only saved the best models (on the validation set) the\n",
        "#last saved model is the best on the validation set.\n",
        "model = keras.models.load_model(\"my_mnist_model.h5\")\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_learning_loss(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCV8pwVzl9GK",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2.3: Model Selection\n",
        "\n",
        "Hyperparameters are the parameters of the learning method itself which we have to specify a priori, i.e., before model fitting. In contrast, model parameters are parameters which arise as a result of the fit. The aim of the model selection is selecting the best hyperparameters for our deep network. Finding the right hyperparameters for a model can be crucial for the model performance on given data. For istance lets consider our model trained by using different values for the learnign rate: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw8Hs-AEmLKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rates = [1e-4, 1e-2, 1e-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mym4GOgUmWqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "histories = []\n",
        "for learning_rate in learning_rates:\n",
        "    model = keras.models.Sequential([\n",
        "                                    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "                                    keras.layers.Dense(300, activation=\"relu\"),\n",
        "                                    keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(lr=learning_rate),\n",
        "              metrics=[\"accuracy\"])\n",
        "              \n",
        "    callbacks = [keras.callbacks.EarlyStopping(patience=2)]\n",
        "\n",
        "    history = model.fit(X_train_scaled, y_train,\n",
        "                        validation_data=(X_valid_scaled, y_valid), epochs=10,\n",
        "                        callbacks=callbacks)\n",
        "    histories.append(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzTSjfwptkHJ",
        "colab_type": "text"
      },
      "source": [
        "Let's plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIlN1vlioIYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for learning_rate, history in zip(learning_rates, histories):\n",
        "    print(\"Learning rate:\", learning_rate)\n",
        "    plot_learning_curves(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atzcLJWuIE1Q",
        "colab_type": "text"
      },
      "source": [
        "Note that the learning rate influence significantly the training phase. That behavior suggests that is crucial to validate the hyper-parameters to obtain good results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JntujSWq3e5",
        "colab_type": "text"
      },
      "source": [
        "### GRID Search:\n",
        "\n",
        "Since a deep net has many hyperparameters, in order to find the best ones, we have to consider all the possible combinations of all of the possible values. One common method to perform this complex task is Grid-Search.\n",
        "Given a set of values for each hyper-parameter, Grid-Search will build a model on each parameter combination possible. It iterates through every parameter combination and stores a model for each combination. Finally, the model that obtained the best result on the validation set will be select.\n",
        "\n",
        "In order to perfrom Grid-Search we will use GridSearchCV method from scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPz629YSrDhN",
        "colab_type": "text"
      },
      "source": [
        "Let's Create a build_model() function that takes two arguments, n_units and learning_rate, and builds, compiles and returns a model with the given number of of neurons and the given learning rate.\n",
        "In order to limit the time demends of the proces we will consider only this two hyper-paramters and a small 3 layer model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGSKWTTtqxZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(n_units=30, learning_rate=1e-3):\n",
        "\n",
        "  #To complete:\n",
        "  #HINT: the function has to build the model with 3 layers:\n",
        "  #- a Flatten layer to convert each 28x28 image to a single row \n",
        "  #- a Danse layer composed of n_units and that exploits Relu activation function \n",
        "  #- the output layer that uses the softmax activation function\n",
        "  #Then, has to be defined how the model will be compiled.\n",
        "    \n",
        "    return model\n",
        "\n",
        "    \n",
        "#Create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model \n",
        "#function to the constructor. This gives you a Scikit-Learn compatible predictor\n",
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS6W3A2LxDg5",
        "colab_type": "text"
      },
      "source": [
        "Let's define the lists of hyper-parameters' values. Also in this case, we use a very limited size lists, but in a real-world scenario a reasonable amount of possible values should be considered (try to add some values and check how much the time required to perform the Grid-Search increases)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-uldCW3ru24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_distribs = {\n",
        "    \"n_units\": [To_complete],# To_complete: insert a few (2 or 3) reasonable values  \n",
        "    \"learning_rate\": [To_complete] # To_complete: insert a few (2 or 3) reasonable values\n",
        "    #Check how much increase the time required to perform GRID search increasing\n",
        "    #the number of the values for each hyper-parameter."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEiBxVYtx_sK",
        "colab_type": "text"
      },
      "source": [
        "Use a sklearn.model_selection.GridSearchCV to search the hyperparameter space of your KerasRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iobF2Bvdr9ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(keras_reg, param_distribs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVGMcZz_yKSX",
        "colab_type": "text"
      },
      "source": [
        "Run the Grid-Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BA3vgTjr-hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search.fit(X_train_scaled, y_train, epochs=10, #The number of epochs can be modified (check what happens by increasing it) \n",
        "                  validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aadmkiOoyM94",
        "colab_type": "text"
      },
      "source": [
        "Print the best hyper-parameters, and evealuate the best model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0nL5OZkylNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(grid_search.best_params_)\n",
        "\n",
        "model = grid_search.best_estimator_.model\n",
        "model.evaluate(X_test_scaled, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}